2:I[2972,["972","static/chunks/972-fe008c56cc430895.js","101","static/chunks/app/projects/%5Bslug%5D/page-c7459cd2fe8431db.js"],""]
3:I[4707,[],""]
5:I[6423,[],""]
4:["slug","llm-eval-workbench","d"]
0:["rXV8BhFwc9yPa4YEO0KlS",[[["",{"children":["projects",{"children":[["slug","llm-eval-workbench","d"],{"children":["__PAGE__?{\"slug\":\"llm-eval-workbench\"}",{}]}]}]},"$undefined","$undefined",true],["",{"children":["projects",{"children":[["slug","llm-eval-workbench","d"],{"children":["__PAGE__",{},[["$L1",["$","main",null,{"className":"section py-16 sm:py-24","children":[["$","$L2",null,{"href":"/","className":"text-sm text-accent","children":"← Back to Projects"}],["$","header",null,{"className":"mt-4","children":[["$","h1",null,{"className":"text-3xl font-semibold","children":"LLM Evaluation Workbench"}],["$","p",null,{"className":"mt-2 text-sm text-muted","children":["2024"," • ","Analytics"]}],["$","div",null,{"className":"mt-3 flex flex-wrap gap-2","children":[["$","span","Python",{"className":"tag","children":"Python"}],["$","span","Evaluation",{"className":"tag","children":"Evaluation"}],["$","span","Benchmarking",{"className":"tag","children":"Benchmarking"}],["$","span","RLHF",{"className":"tag","children":"RLHF"}],["$","span","Analytics",{"className":"tag","children":"Analytics"}]]}]]}],["$","section",null,{"className":"mt-8 grid gap-6","children":[["$","div",null,{"className":"card p-5","children":[["$","h2",null,{"className":"text-lg font-medium","children":"Overview"}],["$","p",null,{"className":"mt-2 text-foreground/85","children":"While supporting supervised fine-tuning and RLHF initiatives, I created a centralized evaluation playbook so reviewers could deliver defensible, repeatable model assessments."}]]}],["$","div",null,{"className":"grid gap-4 sm:grid-cols-3","children":[["$","div",null,{"className":"card p-5 sm:col-span-2","children":[["$","h3",null,{"className":"text-base font-medium","children":"Problem"}],["$","p",null,{"className":"mt-2 text-foreground/85","children":"Evaluation signals were noisy and hard to compare across tasks, which made it difficult to prove model improvements."}]]}],["$","div",null,{"className":"card p-5","children":[["$","h3",null,{"className":"text-base font-medium","children":"Impact"}],["$","p",null,{"className":"mt-2 text-foreground/85","children":"Reduced evaluation errors, improved reasoning quality, and gave product teams trustworthy data to ship milestones with confidence."}]]}]]}],["$","div",null,{"className":"card p-5","children":[["$","h3",null,{"className":"text-base font-medium","children":"Solution"}],["$","p",null,{"className":"mt-2 text-foreground/85","children":"Defined rubric scorecards, created rationale templates, and mapped every task to benchmarking accuracy metrics and client objectives."}]]}],["$","div",null,{"className":"card p-5","children":[["$","h3",null,{"className":"text-base font-medium","children":"Responsibilities"}],["$","ul",null,{"className":"mt-2 list-disc pl-5 text-foreground/85 space-y-1","children":[["$","li","Mapped evaluation workflows to benchmarking targets and business goals",{"children":"Mapped evaluation workflows to benchmarking targets and business goals"}],["$","li","Produced detailed rationales for every scored sample to eliminate ambiguity",{"children":"Produced detailed rationales for every scored sample to eliminate ambiguity"}],["$","li","Aligned reviewers, PMs, and researchers on what “good” looked like for each release",{"children":"Aligned reviewers, PMs, and researchers on what “good” looked like for each release"}]]}]]}],"$undefined"]}]]}],null],null],null]},[null,["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","projects","children","$4","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}]],null]},[null,["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","projects","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}]],null]},[[null,["$","html",null,{"lang":"en","children":["$","body",null,{"children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":"404"}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],"notFoundStyles":[]}]}]}]],null],null],["$L6",null]]]]
6:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"Next.js"}],["$","meta","3",{"name":"description","content":"Generated by Next.js"}]]
1:null
